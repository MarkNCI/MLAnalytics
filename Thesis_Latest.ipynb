{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MarkNCI/MLAnalytics_Exp/blob/main/Thesis_Latest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers -q\n",
        "!pip install streamlit -q\n",
        "!pip install PyMuPDF -q"
      ],
      "metadata": {
        "id": "ZqkaZlDF3mXx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit --version"
      ],
      "metadata": {
        "id": "Xqjm9fXsCjxt",
        "outputId": "b2846b3b-3628-4419-85f4-f98cb79907f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Streamlit, version 1.36.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pymupdf\n",
        "# import spacy"
      ],
      "metadata": {
        "id": "UZQZS6mgfL9_"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_name = '/content/Operational_range_bounding_of_spectroscopy_models_with_anomaly_detection_draft.pdf'"
      ],
      "metadata": {
        "id": "3mAHpZoJWHC0"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def pdf_to_dataframe(input_file):\n",
        "  metadata = []\n",
        "  pdf = pymupdf.open(input_file)\n",
        "  # Get page content as dict and blocks of data\n",
        "  for page in pdf:\n",
        "    pages = page.get_text('dict')\n",
        "    blocks = pages['blocks']\n",
        "    # Extract metadata from blocks and filter for text, font size and font name\n",
        "    for block in blocks:\n",
        "      if 'lines' in block.keys():\n",
        "        spans = block['lines']\n",
        "        for span in spans:\n",
        "          data = span['spans']\n",
        "          for x in data:\n",
        "            metadata.append((x['text'], x['size'], x['font']))\n",
        "  pdf.close()\n",
        "  df = pd.DataFrame(metadata,columns = ['text','size','font'])\n",
        "  return df"
      ],
      "metadata": {
        "id": "w9Pfd3B4dkn9"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pdf_to_dataframe(file_name)\n",
        "print(df['font'].value_counts())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RTjNz5nvjzbh",
        "outputId": "f9963d32-5b7b-4c88-de0b-a1000aebae2e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "font\n",
            "Kp-Regular              468\n",
            "DejaVuSans              156\n",
            "Kp-Medium                63\n",
            "Kp-Italic                55\n",
            "Kp-Expert-Regular        26\n",
            "Kp--M-Italic             22\n",
            "DejaVuSans-Bold          12\n",
            "DejaVuSans-Oblique        7\n",
            "Kp--M-Sy-Regular          5\n",
            "Tt-Kp-Regular             4\n",
            "Kp-Companion-Regular      2\n",
            "Cmsy10                    2\n",
            "Kp-Expert-Medium          1\n",
            "Kp--M-Ex-Regular          1\n",
            "MSBM10                    1\n",
            "Kp-SmallCaps-Regular      1\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Kp-Regular holds actual text so filter for that\n",
        "df = df.loc[df['font'] == 'Kp-Regular']\n",
        "# Column to list for preprocessing\n",
        "corpus = ' '.join(df['text'])\n",
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "XAu_S9AorZhF",
        "outputId": "3727606d-d40e-4131-8084-e53743613a1a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'DRAFT version. Last modified: 2024-05-30 22:03:17Z Operational range bounding of spectroscopy models with anomaly detection Luís F. Simões 1 , Pierluigi Casale 1 , Marília Felismino 1 , Kai Hou Yip 2 , Ingo P. Waldmann 2 , Giovanna Tinetti 2 , Theresa Lueftinger 3 1 2 3 As Machine Learning adoption grows across risk- averse industries such as aerospace, various e orts are underway to design the validation processes that will ensure safety in complex operational settings [1–5].  being advocated by such e orts consists of a safety mechanism that continu- ously monitors system observables, and intervenes if it judges the model will exceed the bounds of a safe domain for a particular input. Unlike monitor- ing systems that run asynchronously from the pre- diction model and perform evaluations over assem- bled production data [6], the safety cage architecture runs a monitoring system in parallel with the predic- tion model, and operates at an individual sample level. This places explicit boundaries over a model’s opera- tional range, something they traditionally lack. Atmospheres of exoplanets are studied using transit spectroscopy. As a planet passes through the projected Corresponding author. E-Mail: luis.simoes@mlanalytics.pt surface of the host star, the observed light curve will display a temporary reduction (a dip) in the overall flux from the planetary system. The level of reduction varies in di erent wavelength channels, which is key to revealing the chemical and dynamical properties of the planet’s atmosphere. These changes are typically 100 ppm or less in magnitude, making them easily buried under di erent instrumental and astrophysical noises orders of magnitude higher than the signal. The Ariel Mission is a European Space Agency (ESA) M4 mission that will conduct the first compre- hensive study of 1000 exoplanets in our galactic neigh- bourhood [7, 8]. The Ariel Data Challenge [9–11] is a series of data competitions organised by mission sci- entists and engineers within the Ariel Consortium to invite innovative solutions to challenging problems faced by the ESA Ariel mission, and by extension, the Planetary Science community. The 2019 and 2021 edi- tions of the challenge [9] looked for innovative de- trending solutions to extract planetary spectra from observed spectroscopic light curves that are contam- inated by the instrument’s systematic noise e ects, as well as stellar activities from the host stars. The dataset simulations represent Ariel observations with mission-dedicated radiometric models to ensure good representation of the instrument. The simulations can however never represent the actual data, which will su er from unknown unknowns. Ensuring the reli- able operation of Machine Learning models, as they move from simulated to real data, with unknowable ground truths, is of paramount importance.  We conduct our experiments over the datasets from the 2019 and 2021 Ariel Data Chal- lenges, hereby designated as ADC19 and ADC21 [9]. DRAFT version. Last modified: 2024-05-30 22:03:17Z  Transit light curves for the same star/planet system, produced by the simulation pipelines [12–15] of the ADC19 (left) and ADC21 (middle) challenges (seen: 55 light curves, one per wavelength channel, each transformed through a sliding-window aggregation over time and across the 10 photon noise instances of the star’s first stellar spot configuration). Right plot: the planet’s transmission spectrum (55 channels) the trained model needs to predict from the 55 light curves. Simulations for exoplanet Qatar-4b, using star/planet parameters from the Ariel Mission Candidate Sample [16, 17]. The 2019 challenge focused solely on stellar noise in the simulations but assumed the instrument was perfect. In 2021 the signal was convolved with the non-linear and time-dependent instrument response models, introducing detector persistence and non- Gaussian red-noise e ects. From a modelling per- spective, those di erences enable the assessment of model performance under conditions of data drift (also known as covariate shift) and concept drift [19]. We use the train data that was made publicly avail- able for those competitions. They contain simulations of 1468 and 1256 star/planet systems, respectively. Experiments across datasets are restricted to a core of 851 systems that were simulated in both datasets, with the same star/planet parameters (Figure 1). The data generation process simulated 10 di erent stellar spot configurations per system, which pollute the sig- nal and therefore have to be identified and corrected for in the derivation of the spectra. Each stellar spot configuration was further corrupted by 10 di erent instances of additive Gaussian photon noise.  We treat the photon noise instances as repeated observations of the same system, and aggregate data across them. We make use of the fact that the known planet parameters allow for alignment of independent observations’ time se- ries, leaving us in this case with an expectation that the maximum dip in the transit light curve will oc- cur around the  = 150 time-step. We implement a \"mirror-stacking\" approach where the observation ma- trix is stacked with a time-inverted copy of itself, thus exploiting the light curve’s expected symmetry (at time  = 140, for instance, we’ll have as well the obser- vations taken at  = 160). A regular grid of 11 points is taken across the time series, every 5 time steps be- tween  = 100 and  = 150. Per grid point, an aggre- gation is performed using a Harmonic mean across the observations in a time window of radius  = 10, and across the same windows of the 10 photon noise instances. Each encoded value then results from aggre- gating across ( 2+1) 2 10 = 420 observations. At this point, data is transformed from relative flux values into relative radii, using the  = 1 (1  ) relation. This has the advantage of simplifying the modelling task, as both model inputs and outputs will now be in the same space. This process is repeated across observations’ 55 wavelength channels, leaving us with a 11 55 = 605 dimensional data encoding. Fi- nally, we stack onto this representation the 6 known star and planet parameters (stellar temperature, sur- face gravity, etc.). By following this process, the raw data is transformed into a matrix  of 12560 611- dimensional samples, in the case of ADC21 data, or 8510 samples in experiments across datasets.  For the sake of simplicity, and to obtain models having abundant failure contexts, we modelled the problem of extracting transmission spec- tra from the previously described data using Ridge re- gression (linear least squares with 2  regularisation). The modelling of data distributions for the purpose of anomaly detection used Isolation Forest models [20, 21]. These models allow us to identify samples that belong to low density region of the training data. Scarcity of data in such regions leads one to assume the predictive model (Rigde in this case) won’t have been su ciently informed as to how to model the tar- get, and its performance will then be expected to be worse there. Isolation Forests produce anomaly scores for the data samples they are given. Values above 0.0 are judged to be inliers with respect to the training DRAFT version. Last modified: 2024-05-30 22:03:17Z  Relation between anomaly scores produced by Isolation Forests and predictive performance of another model trained over the same data. Results obtained through 10-fold cross-validation over the ADC21 dataset. Plots show values measured across the validation-folds for the whole dataset, from Isolation Forest and Ridge models trained over the re- spective train-folds. In the left plot, the Isolation Forest models the distribution of , the 611-dimensional representations of light curves the Ridge model takes as inputs. In the central plot the modelled distribution is that of , the spectra Ridge models are trained to predict. In the right plot the Isolation Forest models the  values produced by the SHAP Explainable AI library [18] that represent how the Ridge model transforms  into . data, and values below that outliers. As we are deal- ing with high dimensional datasets, Isolation Forests were set to model data projections into a lower 30- dimensional space. Those projections were obtained using Principal Component Analysis (PCA). We assess the e ectiveness of three di erent anomaly detection setups, that di er in the data Isola- tion Forests, and PCA, were asked to model (Figure 2): • : default setup, models the data the predictive model takes as input (light curve encodings here). • : this setup ignores the input data, and di- rectly models the transmission spectra. The intu- ition being that if the other model is generating outputs unlike anything that exists in the train- ing data, it will then likely be making mistakes. • : a challenge in tying anomaly scores to an- other model’s predictive performance lies in that the anomaly detection model will lack sensitiv- ity to the kinds of data shifts that most impact the other model. One model treats all dimen- sions as equally important. The other doesn’t. In this setup the Isolation Forest then models the SHAP values (\"SHapley Additive exPlanations\") produced by the SHAP Explainable AI library [18] that represent how the Ridge model trans- forms  into . This being a multi-output re- gression problem, SHAP generates here 55 vec- tors of 611 SHAP values per input sample. The median is taken across the 55 vectors to obtain the data the Isolation Forest is made to model. We used the Ridge, PCA and Isolation Forest im- plementations available in the  0.24.2 library [22], with their default parameter settings. SHAP values obtained with the  0.39.0 library [18], using a  with interventional feature perturbation.  Experiments carried out us- ing Group 10-fold cross-validation. The data pre- processing described above results in datasets with groups of 10 samples belonging to the same planet, having di erent stellar spot noise configurations. Cross-validation then ensured samples belonging to the same planet would always be together in either a train- or validation-fold. Experiments across the ADC19 and ADC21 datasets, having fewer planets, used instead 5-fold CV to increase the volume and di- versity of validation folds. In those experiments, plan- ets in the train-fold were modelled using encodings of ADC21 data, but planets placed the validation-folds used instead encodings of ADC19 data. This allows us to assess e ectiveness of the propossed approaches in contexts of degraded model performance due to drift. Data standardisation into zero mean and unit vari- ance was implemented, as these are important pre- processing steps for Ridge and PCA. Standardisation was defined over each train-fold, and then applied there and in the corresponding validation fold. Predictive performance was assessed using the Root Mean Squared Error (RMSE) metric, chosen for its sen- sitivity to large errors, thus expressing the preference for all values in a spectrum to be accurately predicted.  With the previously presented setup, Ridge models achieve a RMSE of 0.006085 when cross- validating across the ADC21 dataset, and a degrada- DRAFT version. Last modified: 2024-05-30 22:03:17Z  Trade-o s achievable between model coverage and predictive performance when acceptance thresholds are defined over the anomaly scores seen in Figure 2. tion to 0.008421 when cross-validation trains over ADC21 and validates over ADC19. The di erent num- ber of planets across these setups impacts these mea- surements, but they are in line with the performance degradation seen when training over the full ADC21 dataset, and then evaluating over the full ADC19 dataset, where we observe a RMSE of 0.008589. Figure 2 shows the anomaly scores measured across the validation folds having a moderate Spearman’s rank correlation with the RMSE values taken over the sample’s 55 wavelength channels. The strongest corre- lation is observed in the  setup, which validates the hypothesis that aligning the modelled distribution with the behaviour of the predictive model improves the capacity to identify performance degradation. Figure 3 shows the e ectiveness of a Safety Cage architecture that delimits the predictive model’s oper- ational range by imposing acceptance thresholds over the anomaly scores obtained through the approaches presented here. It shows the coverage  error trade- o s achievable as that threshold varies across the span of anomaly scores observed in the validation folds. Again we see the  setup as being the most e ec- tive one. It presents the slowest curve growth, which corresponds to an operational range boundary that is more accurate at including within it the performant regions, and leaving outside the error-prone ones. The right plot shows that even though the transition to ADC19 data entails a large performance degradation overall, that is due to large errors in samples that are easily flagged by the Isolation Forest. An acceptance threshold that would reduce coverage by a small per- centage would already reduce RMSE by 20%. In missions like Ariel, prediction failures by Machine Learning models pose risks, but also opportunities. Data products obtained through ML-assisted process- ing of the mission’s data cannot be contaminated by wrong extrapolations beyond models’ training data. The architecture explored here succeeds at provid- ing safeguards against such risks. We have tackled the problem of identifying out-of-domain samples, which enables us to recognise contexts of high epis- temic uncertainty. As future work we plan to extend the approach with uncertainty quantification meth- ods, which will provide identification as well for con- texts of high aleatoric uncertainty [23]. Beyond such risks, there are also opportunities. Anomaly/novelty detection may point to observation targets of particular scientific interest, due to their uniqueness. Current modelling failures are not guar- anteed to remain as failures if subsequent e ort is al- located to better simulate and understand the failure cases. Some failures result from sparsity in the train- ing data that doesn’t allow the modelling process to capture the patterns of how to process such data. This sort of feedback is valuable to the e ort of gradually building the best possible simulations of the targets of interest, which improves models’ training datasets. The gaps in coverage left by delimiting a model’s op- erational range may also be filled by di erent models, potentially of di erent kinds, that specialise at accu- rate prediction where previous models fail. The cur- rent architecture may also assist such e orts. DRAFT version. Last modified: 2024-05-30 22:03:17Z Research funded by the ESA Science Faculty Research projects \"Machine Learning Quality Assurance in Ariel and beyond\" and \"Ariel Machine Learning Data\". 1. Tambon, F.  How to certify machine learning based safety- critical systems? A systematic literature review.  38 (2022). 2. Mamalet, F.  Research Report (IRT Saint Exupéry ; ANITI, Mar. 2021). . 3. ECSS-European Cooperation for Space Standardization. Public Review ECSS-E-HB-40-02A DIR1 (ESA Requirements and Standards Section, May 2023). 4. MLEAP Consortium.  Horizon Europe research and innovation programme report (Euro- pean Union Aviation Safety Agency, May 2023). 5. Borg, M.  Safely Entering the Deep: A Review of Verifi- cation and Validation for Machine Learning and a Challenge Elicitation in the Automotive Industry.  1–19 (1 2019). 6. Treveil, M. : 9781492083290 (O’Reilly Me- dia, Incorporated, 2020). 7. Tinetti, G.  A chemical survey of exoplanets with ARIEL.  135–209 (2018). 8. Tinetti, G.  Ariel: Enabling planetary science across light- years.  (2021). 9. Nikolaou, N.  Lessons learned from the 1st Ariel Ma- chine Learning Challenge: Correcting transiting exoplanet light curves for stellar spots.  695–709 (2023). 10. Changeat, Q. & Yip, K. H. ESA-Ariel Data Challenge NeurIPS 2022: introduction to exo-atmospheric studies and presenta- tion of the Atmospheric Big Challenge (ABC) Database.  45–61 (Jan. 2023). 11. Yip, K. H.  in  (PMLR, Nov. 2022), 1–17. 12. Morello, G.  The ExoTETHyS package: tools for exoplan- etary transits around host stars. 75 (2020). 13. Waldmann, I. P.  Tau-REx I: A next generation retrieval code for exoplanetary atmospheres.  107 (2015). 14. Mugnai, L. V., Pascale, E., Edwards, B., Papageorgiou, A. & Sarkar, S. ArielRad: the Ariel radiometric model.  303–328 (2020). 15. Sarkar, S., Pascale, E., Papageorgiou, A., Johnson, L. J. & Wald- mann, I. ExoSim: the exoplanet observation simulator.  287–317 (2021). 16. Edwards, B., Mugnai, L., Tinetti, G., Pascale, E. & Sarkar, S. An updated study of potential targets for Ariel.  242 (2019). 17. Edwards, B. & Tinetti, G. The Ariel Target List: The Impact of TESS and the Potential for Characterizing Multiple Planets within a System.  15 (2022). 18. Lundberg, S. M. & Lee, S. - I.  in  (eds Guyon, I. ) (2017), 4765–4774. 19. Bayram, F., Ahmed, B. S. & Kassler, A. From concept drift to model degradation: An overview on performance-aware drift detectors.  108632 (2022). 20. Liu, F. T., Ting, K. M. & Zhou, Z. - H.  in  (2008), 413–422. 21. Liu, F. T., Ting, K. M. & Zhou, Z. - H. Isolation-Based Anomaly Detection.  (2012). 22. Pedregosa, F.  Scikit-learn: Machine Learning in Python.  2825–2830 (2011). 23. Psaros, A. F., Meng, X., Zou, Z., Guo, L. & Karniadakis, G. E. Uncertainty quantification in scientific machine learning: Methods, metrics, and comparisons.  111902 (2023).'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer"
      ],
      "metadata": {
        "id": "m8dU5lrHkHPf"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import textwrap\n",
        "model_name = \"facebook/bart-large-cnn\"\n",
        "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "\n",
        "inputs = tokenizer.encode(\"summarize: \" + corpus, return_tensors=\"pt\", max_length=1024, truncation=True, padding='longest') # Added Padding\n",
        "summary_ids = model.generate(inputs, max_length=150, min_length=50, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
        "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
        "formatted_summary = \"\\n\".join(textwrap.wrap(summary, width=80))"
      ],
      "metadata": {
        "collapsed": true,
        "id": "JpNkSuFsmAgs"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_list = [\"The Ariel Data Challenge [9–11] is a series of data competitions organised by mission scientists and engineers within the Ariel Consortium. The 2019 and 2021 editions of the challenge looked for innovative detrending solutions to extract planetary spectra from observed spectroscopic light curves. The simulations can however never represent the actual data, which will su er from unknown unknowns. Ensuring the reliable operation of Machine Learning models, as they move from simulated to real data, with unknowable ground truths, is of paramount importance. We conduct our experiments over the datasets from the 2019 and 2019 Ariel Data Challenges, hereby designated as ADC19 and ADC21 [9]. DRAFT version. Last modified: 2024-05-30 22:03:17Z  Transit light curves for the same star/planet system, produced by the simulation pipelines [12–15] of the ADC19 or ADC21 challenges. We treat the photon noise instances of the star’s first stellar spot configuration as repeated observations and aggregate them across the same system. The data process simulated 10 stellar spot configurations, which have to be identified and corrected in the derivation of the derivations. We use the train data that was made for those competitions. Experiments are restricted to a core of 1468 and 1256 star/Planet systems, respectively, that were simulated in both datasets, with the sameStar/Planet parameters. We treated the photon Noise instances as repeated Observations of the same System, and corrected them in the derived derivation. The results were published in the journal The Astrophysical Journal [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 34, 33, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 59,. 60, 61, 62, 63, 64, 63,. 64, 65, 68, 69, 70, 74, 75, 80, 78, 81, 82, 83, 84, 84,. 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 94,. 94, 95, 96, 97, 98, 99, 100, 102, 103, 104, 105, 106, 108, 109, 110, 111, 112, 113, 114, 116, 113,. 111, 114,. 113, 116,. 116, 117, 118, 119, 113 and 114, 118,. 119, 120, 123, 123,. 130, 130, 116 and 123, 124, 129, 130,. 131, 128, 131, 134, 135, 135,. 131,. 130,. 135, 134,. 137, 137, 138, 139, 140, 145, 149, 151, 152, 163, 164, 165, 168, 169, 173, 174, 175, 178, 179, 194, 199, 204, 199,. 207, 204,. 204, 205, 206, 207, 208, 209, 210, 217, 209,. 208, 204. 211, 209. 204, 215, 215,. 216, 217,. 217, 208,. 215, 217. 224, 215. 226, 215., 216, 209., 215, 216, 220, 217; 217, 224, 223, 226, 233, 220,. 215,. 217,. 224, 226,. 215. 217, 227, 224,. 220, 227,. 226, 224. 215, 226. 215,. 220,. 226,. 227, 227. 226,. 220. 227, 233,. 224,. 224. 224,. 226. 227,. 227,. 230, 220. 229, 226., 224, 224; 215, 227; 226, 229, 229,. 220; 220, 233; 224, 230, 230,. 217. 215. 224. 236, 227., 227, 230. 226. 226., 230, 229. 227. 230, 233. 236. 227., 220, 226; 227, 238, 233., 227,. 220, 230, 238,. 230,. 230, 233, 230, 227, 236, 230; 238, 236,. 227, 238, 238. & 230, 236. & 236, 233, 236, 236 (including the data for Qatar-4b) is a set of data that will be used to study exoplanets in our galactic neighbourhood. As a planet passes through the projected surface of the host star, the observed light curve will display a temporary reduction (a dip) in the overall flux from the planetary system. This level of reduction varies in di erent wavelength channels, which is key to revealing the chemical and dynamical properties of the planet's atmosphere.\"]"
      ],
      "metadata": {
        "id": "-an51kxyY_9M"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "pattern = r\"\\[.*?\\]\"\n",
        "\n",
        "out = []\n",
        "for name in text_list:\n",
        "    if '[' in name and ']' in name:\n",
        "      name = name[:name.find('[')] + name[name.find(']')+1:]\n",
        "      out.append(name)\n",
        "    print(out)\n",
        "    cleaned_text = re.sub(\" \\d+\", \" \", str(out))\n",
        "    cleaned_text = re.sub('[^A-Za-z0-9]+', \" \", str(cleaned_text))\n",
        "    print(cleaned_text)"
      ],
      "metadata": {
        "id": "YJflQ0AcZs_u",
        "outputId": "868bcd29-308a-4ffb-e6aa-fba659c7e86d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\"The Ariel Data Challenge  is a series of data competitions organised by mission scientists and engineers within the Ariel Consortium. The 2019 and 2021 editions of the challenge looked for innovative detrending solutions to extract planetary spectra from observed spectroscopic light curves. The simulations can however never represent the actual data, which will su er from unknown unknowns. Ensuring the reliable operation of Machine Learning models, as they move from simulated to real data, with unknowable ground truths, is of paramount importance. We conduct our experiments over the datasets from the 2019 and 2019 Ariel Data Challenges, hereby designated as ADC19 and ADC21 [9]. DRAFT version. Last modified: 2024-05-30 22:03:17Z  Transit light curves for the same star/planet system, produced by the simulation pipelines [12–15] of the ADC19 or ADC21 challenges. We treat the photon noise instances of the star’s first stellar spot configuration as repeated observations and aggregate them across the same system. The data process simulated 10 stellar spot configurations, which have to be identified and corrected in the derivation of the derivations. We use the train data that was made for those competitions. Experiments are restricted to a core of 1468 and 1256 star/Planet systems, respectively, that were simulated in both datasets, with the sameStar/Planet parameters. We treated the photon Noise instances as repeated Observations of the same System, and corrected them in the derived derivation. The results were published in the journal The Astrophysical Journal [7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 34, 33, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 59,. 60, 61, 62, 63, 64, 63,. 64, 65, 68, 69, 70, 74, 75, 80, 78, 81, 82, 83, 84, 84,. 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 94,. 94, 95, 96, 97, 98, 99, 100, 102, 103, 104, 105, 106, 108, 109, 110, 111, 112, 113, 114, 116, 113,. 111, 114,. 113, 116,. 116, 117, 118, 119, 113 and 114, 118,. 119, 120, 123, 123,. 130, 130, 116 and 123, 124, 129, 130,. 131, 128, 131, 134, 135, 135,. 131,. 130,. 135, 134,. 137, 137, 138, 139, 140, 145, 149, 151, 152, 163, 164, 165, 168, 169, 173, 174, 175, 178, 179, 194, 199, 204, 199,. 207, 204,. 204, 205, 206, 207, 208, 209, 210, 217, 209,. 208, 204. 211, 209. 204, 215, 215,. 216, 217,. 217, 208,. 215, 217. 224, 215. 226, 215., 216, 209., 215, 216, 220, 217; 217, 224, 223, 226, 233, 220,. 215,. 217,. 224, 226,. 215. 217, 227, 224,. 220, 227,. 226, 224. 215, 226. 215,. 220,. 226,. 227, 227. 226,. 220. 227, 233,. 224,. 224. 224,. 226. 227,. 227,. 230, 220. 229, 226., 224, 224; 215, 227; 226, 229, 229,. 220; 220, 233; 224, 230, 230,. 217. 215. 224. 236, 227., 227, 230. 226. 226., 230, 229. 227. 230, 233. 236. 227., 220, 226; 227, 238, 233., 227,. 220, 230, 238,. 230,. 230, 233, 230, 227, 236, 230; 238, 236,. 227, 238, 238. & 230, 236. & 236, 233, 236, 236 (including the data for Qatar-4b) is a set of data that will be used to study exoplanets in our galactic neighbourhood. As a planet passes through the projected surface of the host star, the observed light curve will display a temporary reduction (a dip) in the overall flux from the planetary system. This level of reduction varies in di erent wavelength channels, which is key to revealing the chemical and dynamical properties of the planet's atmosphere.\"]\n",
            " The Ariel Data Challenge is a series of data competitions organised by mission scientists and engineers within the Ariel Consortium The and editions of the challenge looked for innovative detrending solutions to extract planetary spectra from observed spectroscopic light curves The simulations can however never represent the actual data which will su er from unknown unknowns Ensuring the reliable operation of Machine Learning models as they move from simulated to real data with unknowable ground truths is of paramount importance We conduct our experiments over the datasets from the and Ariel Data Challenges hereby designated as ADC19 and ADC21 9 DRAFT version Last modified 05 30 03 17Z Transit light curves for the same star planet system produced by the simulation pipelines 12 15 of the ADC19 or ADC21 challenges We treat the photon noise instances of the star s first stellar spot configuration as repeated observations and aggregate them across the same system The data process simulated stellar spot configurations which have to be identified and corrected in the derivation of the derivations We use the train data that was made for those competitions Experiments are restricted to a core of and star Planet systems respectively that were simulated in both datasets with the sameStar Planet parameters We treated the photon Noise instances as repeated Observations of the same System and corrected them in the derived derivation The results were published in the journal The Astrophysical Journal 7 and and including the data for Qatar 4b is a set of data that will be used to study exoplanets in our galactic neighbourhood As a planet passes through the projected surface of the host star the observed light curve will display a temporary reduction a dip in the overall flux from the planetary system This level of reduction varies in di erent wavelength channels which is key to revealing the chemical and dynamical properties of the planet s atmosphere \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "regex = [' \\d+', ]\n",
        "\n",
        "# Use re.sub() to remove the matched pattern\n",
        "cleaned_text = re.sub(\" \\d+\", \" \", str(text_list))\n",
        "cleaned_text = re.sub('[^A-Za-z0-9]+', \" \", str(cleaned_text))\n",
        "print(cleaned_text)"
      ],
      "metadata": {
        "id": "fwzbtrPlrAZl",
        "outputId": "30f9259c-6e0c-4a37-8d02-ac07f6cb7ae1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The Ariel Data Challenge 9 11 is a series of data competitions organised by mission scientists and engineers within the Ariel Consortium The and editions of the challenge looked for innovative detrending solutions to extract planetary spectra from observed spectroscopic light curves The simulations can however never represent the actual data which will su er from unknown unknowns Ensuring the reliable operation of Machine Learning models as they move from simulated to real data with unknowable ground truths is of paramount importance We conduct our experiments over the datasets from the and Ariel Data Challenges hereby designated as ADC19 and ADC21 9 DRAFT version Last modified 05 30 03 17Z Transit light curves for the same star planet system produced by the simulation pipelines 12 15 of the ADC19 or ADC21 challenges We treat the photon noise instances of the star s first stellar spot configuration as repeated observations and aggregate them across the same system The data process simulated stellar spot configurations which have to be identified and corrected in the derivation of the derivations We use the train data that was made for those competitions Experiments are restricted to a core of and star Planet systems respectively that were simulated in both datasets with the sameStar Planet parameters We treated the photon Noise instances as repeated Observations of the same System and corrected them in the derived derivation The results were published in the journal The Astrophysical Journal 7 and and including the data for Qatar 4b is a set of data that will be used to study exoplanets in our galactic neighbourhood As a planet passes through the projected surface of the host star the observed light curve will display a temporary reduction a dip in the overall flux from the planetary system This level of reduction varies in di erent wavelength channels which is key to revealing the chemical and dynamical properties of the planet s atmosphere \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Streamlit App"
      ],
      "metadata": {
        "id": "Or7GSrik70sc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import os, re\n",
        "from transformers import BartForConditionalGeneration, BartTokenizer\n",
        "from io import StringIO\n",
        "import pandas as pd\n",
        "import pymupdf\n",
        "from datetime import datetime\n",
        "\n",
        "# Intro Text\n",
        "st.markdown('# PDF summarization using BART')\n",
        "st.header('''PDF size must be below 200MB. Only 1 PDF per time, time will vary depending on size''')\n",
        "\n",
        "input_file = st.file_uploader('Upload a PDF file')\n",
        "\n",
        "# Save uploaded file\n",
        "if input_file:\n",
        "  with open(os.path.join(input_file.name),'wb') as f:\n",
        "        f.write(input_file.getbuffer())\n",
        "  st.success('Saved File')\n",
        "\n",
        "st.markdown('# Summary')\n",
        "\n",
        "# Load Model\n",
        "model_name = \"facebook/bart-large-cnn\"\n",
        "model = BartForConditionalGeneration.from_pretrained(model_name)\n",
        "# Load tokeniser\n",
        "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
        "\n",
        "def pdf_to_dataframe(input_file):\n",
        "  metadata = []\n",
        "  pdf = pymupdf.open(input_file)\n",
        "  # Get page content as dict and blocks of data\n",
        "  for page in pdf:\n",
        "    pages = page.get_text('dict')\n",
        "    blocks = pages['blocks']\n",
        "    # Extract metadata from blocks and filter for text, font size and font name\n",
        "    for block in blocks:\n",
        "      if 'lines' in block.keys():\n",
        "        spans = block['lines']\n",
        "        for span in spans:\n",
        "          data = span['spans']\n",
        "          for x in data:\n",
        "            metadata.append((x['text'], x['size'], x['font']))\n",
        "  pdf.close()\n",
        "  output = pd.DataFrame(metadata,columns = ['text','size','font'])\n",
        "  return output\n",
        "\n",
        "if input_file:\n",
        "  start = datetime.now()\n",
        "  st.success('Summary loading, please wait...')\n",
        "  page_wise_text = []\n",
        "  # Kp-Regular holds actual text so filter for that\n",
        "  df = pdf_to_dataframe(input_file)\n",
        "  df = df.loc[df['font'] == 'Kp-Regular']\n",
        "  corpus = ' '.join(df['text'])\n",
        "\n",
        "  # Tokenise text\n",
        "  tokens = tokenizer.encode(corpus, truncation=True, padding='longest', return_tensors='pt',max_length=1024)\n",
        "\n",
        "  # Summarise text\n",
        "  # summary = model.generate(tokens,max_length=1024, min_length=50, length_penalty=0.0, num_beams=4, early_stopping=True)\n",
        "  summary = model.generate(tokens,max_length=1024, min_length=1000, length_penalty=0.0, num_beams=4, early_stopping=True)\n",
        "\n",
        "  # Decode tokens and add to list\n",
        "  decoded_output = tokenizer.decode(summary[0],skip_special_tokens=True)\n",
        "  page_wise_text.append(decoded_output)\n",
        "  # Remove '-'\n",
        "  page_wise_text_cleaned = [ele.replace('- ', '') for ele in page_wise_text]\n",
        "  # Remove brackets with numbers in them\n",
        "  page_wise_text_cleaned = [re.sub(r'\\[.*\\]', ' ', x) for x in page_wise_text_cleaned]\n",
        "  # Remove page/fig numbers and unwanted characters\n",
        "  page_wise_text_cleaned = [re.sub(r' \\d+', ' ', x) for x in page_wise_text_cleaned]\n",
        "  page_wise_text_cleaned = [re.sub(r'[^A-Za-z0-9]+', ' ', x) for x in page_wise_text_cleaned]\n",
        "\n",
        "  # Show Summary\n",
        "  st.success('Summary ready!')\n",
        "  st.text_area(label =\"\",value=page_wise_text_cleaned, placeholder=\"Please upload a PDF to get it's summary\", height = 400)\n",
        "  st.markdown('')\n",
        "  end = datetime.now() - start\n",
        "\n",
        "  st.header(end)"
      ],
      "metadata": {
        "id": "QlLVenql6IDJ",
        "outputId": "a596c6be-a48c-4806-a975-b5ac66817dbd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install -g localtunnel"
      ],
      "metadata": {
        "id": "do8zgXId3qvM",
        "outputId": "94116495-ffc4-46df-d280-49a896d565e6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h/tools/node/bin/lt -> /tools/node/lib/node_modules/localtunnel/bin/lt.js\n",
            "\u001b[K\u001b[?25h+ localtunnel@2.0.2\n",
            "updated 1 package in 1.621s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl ipv4.icanhazip.com"
      ],
      "metadata": {
        "id": "truv2JeL3yF3",
        "outputId": "96850ee9-b5a4-48ec-c609-37c7d812569a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.73.2.159\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py &>./logs.txt & npx localtunnel --port 8501"
      ],
      "metadata": {
        "id": "8uXKXJYF328n",
        "outputId": "4d648047-36c1-4a5a-c417-01642c5d2eed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25hnpx: installed 22 in 3.856s\n",
            "your url is: https://huge-squids-call.loca.lt\n",
            "^C\n"
          ]
        }
      ]
    }
  ]
}