{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPCFwWiKqLLI7WQKkc2pwEU"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers -q\n",
        "!pip install streamlit -q\n",
        "!pip install openai -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5jHrh6L68YE",
        "outputId": "e3e557ee-db4b-443d-c024-86aff4f67f19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m18.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.5/328.5 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oE6FqPD5Yyy",
        "outputId": "1c4edcbe-45db-4b51-ff01-2d3ea1b9a851"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import openai, requests, os\n",
        "from anthropic import Anthropic\n",
        "\n",
        "openai.api_key = ''\n",
        "client = anthropic.Anthropic(\n",
        "    api_key=''\n",
        ")\n",
        "MODEL_NAME = \"claude-3-opus-20240229\"\n",
        "\n",
        "# Intro Text\n",
        "st.markdown('# PDF summarization using Generative AI')\n",
        "st.header('''PDF size must be below 200MB. Only 1 PDF per time, time will vary depending on size''')\n",
        "\n",
        "input_file = st.file_uploader('Upload a PDF file')\n",
        "\n",
        "# Save uploaded file\n",
        "if input_file:\n",
        "  with open(os.path.join(input_file.name),'wb') as f:\n",
        "        f.write(input_file.getbuffer())\n",
        "  st.success('Saved File')\n",
        "\n",
        "st.markdown('# Summary')\n",
        "\n",
        "def summarise_file(prompt,model = 'gpt-3.5-turbo'):\n",
        "  st.success('Summary loading, please wait...')\n",
        "  # Connect to API endpoint\n",
        "  URL = 'https://api.openai.com/v1/chat/completions'\n",
        "\n",
        "  parameters = {\n",
        "      'model':model,\n",
        "      'messages': [{'role':'user', 'content':prompt}],\n",
        "      'temperature': 1,\n",
        "      'top_p': 1,\n",
        "      'frequency_penalty':1,\n",
        "      'presence_penalty':1\n",
        "  }\n",
        "\n",
        "  headers = {\n",
        "      'Content-Type': 'application/json',\n",
        "      'Authorization': f'Bearer {openai.api_key}'\n",
        "  }\n",
        "\n",
        "  response = requests.post(URL, headers=headers, json=parameters, stream=False)\n",
        "  json_response = response.json()\n",
        "  summary = json_response['choices'][0]['message']['content'].strip()\n",
        "  return summary\n",
        "\n",
        "\n",
        "# def keywords(prompt,model = 'gpt-3.5-turbo'):\n",
        "#   # Connect to API endpoint\n",
        "#   URL = 'https://api.openai.com/v1/chat/completions'\n",
        "\n",
        "#   parameters = {\n",
        "#       'model':model,\n",
        "#       'messages': [{'role':'user', 'content':prompt}],\n",
        "#       'temperature': 1,\n",
        "#       'top_p': 1,\n",
        "#       'frequency_penalty':1,\n",
        "#       'presence_penalty':1\n",
        "#   }\n",
        "\n",
        "#   headers = {\n",
        "#       'Content-Type': 'application/json',\n",
        "#       'Authorization': f'Bearer {openai.api_key}'\n",
        "#   }\n",
        "\n",
        "#   response = requests.post(URL, headers=headers, json=parameters, stream=False)\n",
        "#   json_response = response.json()\n",
        "#   summary = json_response['choices'][0]['message']['content'].strip()\n",
        "#   return summary\n",
        "\n",
        "def get_completion(client, prompt):\n",
        "    return client.messages.create(\n",
        "        model=MODEL_NAME,\n",
        "        max_tokens=2048,\n",
        "        messages=[{\n",
        "            \"role\": 'user', \"content\":  prompt\n",
        "        }]\n",
        "    ).content[0].text\n",
        "\n",
        "if input_file:\n",
        "  prompt = f'''\n",
        "  ### TASK\n",
        "  You are an expert in text analysis and creating abstracts from research papers. Your task is to analyse a research paper I'm going\n",
        "  to give you and create an abstract for it.\n",
        "  ### INPUTS\n",
        "  Research Paper: {input_file}\n",
        "  ### OUTPUT\n",
        "  The abstract must answer the following questions: What is the paper about? Why is the paper important? What methods were used?\n",
        "  What industry partners were involved? What are the paper's findings? Why are the papers findings important?\n",
        "  '''\n",
        "  text_area = summarise_file(prompt)\n",
        "\n",
        "  # Show Summary\n",
        "  st.success('Summary ready!')\n",
        "  st.text_area(label =\"\",value= text_area, placeholder=\"Please upload a PDF to get it's summary\", height = 400)\n",
        "  st.markdown('')\n",
        "\n",
        "\n",
        "  # st.success('Recommendations loading, please wait...')\n",
        "\n",
        "  # prompt = f'''\n",
        "  # ### TASK\n",
        "  # You are an expert in text analysis and keyword extraction. Your task is to analyse a research paper I'm going\n",
        "  # to give you and extract from it the top scientific keywords that are most representative of its content. Then you're going to convert the\n",
        "  # keywords to lower case and search the provided URL and return the top 5 results. The url you will use is\n",
        "  # http://export.arxiv.org/api/query?search_query=all:keyword&start=0&max_results=5. If the keyword is more than one word seperate with a\n",
        "  # + symbol such as machine+learning.\n",
        "  # ### INPUTS\n",
        "  # Summary: {input_file}\n",
        "  # ### OUTPUT\n",
        "  # The list of results you're going to generate should be in descending order from the most relevant to the least relevant.\n",
        "  # '''\n",
        "  # test = keywords(prompt)\n",
        "  # # Show Recommendations\n",
        "  # st.success('Recommendations ready!')\n",
        "  # st.text_area(label =\"\",value= test, placeholder=\"Please upload a PDF to get it's summary\", height = 400)\n",
        "  # st.markdown('')\n",
        "\n",
        "  completion = get_completion(client,\n",
        "      f\"\"\"Here is an academic paper: <paper>{input_file}</paper>\n",
        "\n",
        "  Please do the following:\n",
        "  1. Summarise the research paper in such a way that the reader won't need to read the whole paper.\n",
        "  \"\"\"\n",
        "  )\n",
        "  print(completion)\n",
        "  st.text_area(label =\"\",value= completion, placeholder=\"Please upload a PDF to get it's summary\", height = 400)\n",
        "  st.markdown('')\n",
        "\n",
        "else:\n",
        "  st.text_area(label =\"\",value= \"Please upload a PDF to get it's summary\", placeholder=\"Please upload a PDF to get it's summary\", height = 400)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You are an expert in creating abstracts from research papers and finding similar abstracts. Your task is to analyse a research paper I'm going\n",
        "  to give you and create an abstract that is most representative of its content especially the points made in the first paragraph. You will th\n",
        "  The url you will use is http://export.arxiv.org/api/query?search_query=all:keyword&start=0&max_results=5. If the keyword is more\n",
        "  than one word seperate with a + symbol such as machine+learning."
      ],
      "metadata": {
        "id": "iYChkUyCiBpR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TASK\n",
        "  You are an expert in text analysis and summarisation with research papers. Your task is to analyse a research paper I'm going to give you\n",
        "  and provide a detailed summary that is most representative of its content. You must use the Key Concept Clarity method to read the paper and\n",
        "  highlight all of the key talking points of the paper.\n",
        "  ### INPUTS\n",
        "  Research Paper: {input_file}\n",
        "  ### OUTPUT\n",
        "  The output should be structured so as to identify and concisely explain the main ideas in the document. Each section should consist of the\n",
        "  name of the talking point, a detailed definition of the talking point and a short summary of the talking point.\n",
        "  Aim to provide at least four talking points. Here is how each talking point should be structured:\n",
        "  Name of the talking point\n",
        "  Explanation: Define in detail what is the talking point\n",
        "  Overview: Provide a short summary of the talking point including its challenges ,if any, and findings.\n",
        "  '''"
      ],
      "metadata": {
        "id": "HbqIaaV-q_uK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rerun this if error"
      ],
      "metadata": {
        "id": "T-VT_obdVm7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install -g localtunnel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khiFwlxK9FVl",
        "outputId": "dbb35c24-f4ba-44a9-b763-fd81ea1c53da"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h/tools/node/bin/lt -> /tools/node/lib/node_modules/localtunnel/bin/lt.js\n",
            "+ localtunnel@2.0.2\n",
            "updated 1 package in 1.045s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl ipv4.icanhazip.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wQvJHmR8-_J",
        "outputId": "ed373385-dd12-4ea8-bd64-3bd229f4d33c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35.247.29.50\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py &>./logs.txt & npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bf5l7uo19BtD",
        "outputId": "b3244cc0-0205-404c-fccd-adb02fa2202d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25hnpx: installed 22 in 2.603s\n",
            "your url is: https://witty-spoons-say.loca.lt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install anthropic"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKSmeuk5NfSt",
        "outputId": "88d131bd-c49f-456f-e797-fcc4b79fb249"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting anthropic\n",
            "  Downloading anthropic-0.31.0-py3-none-any.whl (865 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.4/865.4 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from anthropic) (1.7.0)\n",
            "Collecting httpx<1,>=0.23.0 (from anthropic)\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jiter<1,>=0.4.0 (from anthropic)\n",
            "  Downloading jiter-0.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (318 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (2.8.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from anthropic) (1.3.1)\n",
            "Requirement already satisfied: tokenizers>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from anthropic) (0.19.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.7 in /usr/local/lib/python3.10/dist-packages (from anthropic) (4.12.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic) (3.7)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->anthropic) (1.2.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->anthropic) (2024.6.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->anthropic)\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->anthropic)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->anthropic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1.9.0->anthropic) (2.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from tokenizers>=0.13.0->anthropic) (0.23.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (3.15.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2023.6.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (4.66.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (3.3.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.0->anthropic) (2.0.7)\n",
            "Installing collected packages: jiter, h11, httpcore, httpx, anthropic\n",
            "Successfully installed anthropic-0.31.0 h11-0.14.0 httpcore-1.0.5 httpx-0.27.0 jiter-0.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from anthropic import Anthropic\n",
        "client = anthropic.Anthropic(\n",
        "    # defaults to os.environ.get(\"ANTHROPIC_API_KEY\")\n",
        "    api_key=''\n",
        ")\n",
        "MODEL_NAME = \"claude-3-opus-20240229\""
      ],
      "metadata": {
        "id": "1low860SQE5R"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion(client, prompt):\n",
        "    return client.messages.create(\n",
        "        model=MODEL_NAME,\n",
        "        max_tokens=2048,\n",
        "        messages=[{\n",
        "            \"role\": 'user', \"content\":  prompt\n",
        "        }]\n",
        "    ).content[0].text"
      ],
      "metadata": {
        "id": "PEDc5OKOQMwz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = '/content/Operational_range_bounding_of_spectroscopy_models_with_anomaly_detection_draft.pdf'"
      ],
      "metadata": {
        "id": "4w1KwYQrRCTR"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "completion = get_completion(client,\n",
        "    f\"\"\"Here is an academic paper: <paper>{text}</paper>\n",
        "\n",
        "Please do the following:\n",
        "1. Summarise the research paper in such a way that the reader won't need to read the whole paper.\n",
        "\"\"\"\n",
        ")\n",
        "print(completion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "7CNsGFI7QWBc",
        "outputId": "61080ccb-b52a-4978-a168-80e63bda6e96"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "error",
          "ename": "BadRequestError",
          "evalue": "Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Your credit balance is too low to access the Claude API. Please go to Plans & Billing to upgrade or purchase credits.'}}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-bac7be09cb8b>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m completion = get_completion(client,\n\u001b[0m\u001b[1;32m      2\u001b[0m     f\"\"\"Here is an academic paper: <paper>{text}</paper>\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mPlease\u001b[0m \u001b[0mdo\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mfollowing\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;36m1.\u001b[0m \u001b[0mSummarize\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mabstract\u001b[0m \u001b[0mat\u001b[0m \u001b[0ma\u001b[0m \u001b[0mkindergarten\u001b[0m \u001b[0mreading\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mIn\u001b[0m \u001b[0;34m<\u001b[0m\u001b[0mkindergarten_abstract\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-2d7ed35ce508>\u001b[0m in \u001b[0;36mget_completion\u001b[0;34m(client, prompt)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     return client.messages.create(\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMODEL_NAME\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mmax_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         messages=[{\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anthropic/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anthropic/resources/messages.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, max_tokens, messages, model, metadata, stop_sequences, stream, system, temperature, tool_choice, tools, top_k, top_p, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTimeout\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mNotGiven\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    903\u001b[0m     ) -> Message | Stream[RawMessageStreamEvent]:\n\u001b[0;32m--> 904\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    905\u001b[0m             \u001b[0;34m\"/v1/messages\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    906\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1264\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m         )\n\u001b[0;32m-> 1266\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1268\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    940\u001b[0m         \u001b[0mstream_cls\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_StreamT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m     ) -> ResponseT | _StreamT:\n\u001b[0;32m--> 942\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    943\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/anthropic/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1044\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1045\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1046\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1047\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m         return self._process_response(\n",
            "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'type': 'error', 'error': {'type': 'invalid_request_error', 'message': 'Your credit balance is too low to access the Claude API. Please go to Plans & Billing to upgrade or purchase credits.'}}"
          ]
        }
      ]
    }
  ]
}