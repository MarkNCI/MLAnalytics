{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers -q\n",
        "!pip install streamlit -q\n",
        "!pip install openai -q\n",
        "!pip install anthropic -q\n",
        "!pip install pypdf -q\n",
        "!pip install pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q5jHrh6L68YE",
        "outputId": "58c862d1-ac59-48b9-f3f6-cbdb92d9b5e8"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m53.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m49.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m337.0/337.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m865.5/865.5 kB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.8/295.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyngrok\n",
            "  Downloading pyngrok-7.2.0-py3-none-any.whl.metadata (7.4 kB)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.1)\n",
            "Downloading pyngrok-7.2.0-py3-none-any.whl (22 kB)\n",
            "Installing collected packages: pyngrok\n",
            "Successfully installed pyngrok-7.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0oE6FqPD5Yyy",
        "outputId": "7b70e5d1-555a-447a-8c3e-009dd5e0435e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import openai, requests, os\n",
        "from anthropic import Anthropic\n",
        "from pypdf import PdfReader\n",
        "\n",
        "# openai.api_key = ''\n",
        "client = Anthropic(\n",
        "    api_key=''\n",
        ")\n",
        "MODEL_NAME = 'claude-3-opus-20240229'\n",
        "# MODEL_NAME = 'claude-3-5-sonnet-20240620'\n",
        "\n",
        "# Intro Text\n",
        "st.markdown('# PDF summarization using Generative AI')\n",
        "st.header('''PDF size must be below 200MB. Only 1 PDF per time, time will vary depending on size''')\n",
        "\n",
        "input_file = st.file_uploader('Upload a PDF file')\n",
        "\n",
        "# Save uploaded file\n",
        "if input_file:\n",
        "  with open(os.path.join(input_file.name),'wb') as f:\n",
        "        f.write(input_file.getbuffer())\n",
        "  st.success('Saved File')\n",
        "\n",
        "st.markdown('# Summary')\n",
        "\n",
        "# def summarise_file(prompt,model = 'gpt-3.5-turbo'):\n",
        "#   st.success('Summary loading, please wait...')\n",
        "#   # Connect to API endpoint\n",
        "#   URL = 'https://api.openai.com/v1/chat/completions'\n",
        "\n",
        "#   parameters = {\n",
        "#       'model':model,\n",
        "#       'messages': [{'role':'user', 'content':prompt}],\n",
        "#       'temperature': 1,\n",
        "#       'top_p': 1,\n",
        "#       'frequency_penalty':1,\n",
        "#       'presence_penalty':1\n",
        "#   }\n",
        "\n",
        "#   headers = {\n",
        "#       'Content-Type': 'application/json',\n",
        "#       'Authorization': f'Bearer {openai.api_key}'\n",
        "#   }\n",
        "\n",
        "  # response = requests.post(URL, headers=headers, json=parameters, stream=False)\n",
        "  # json_response = response.json()\n",
        "  # summary = json_response['choices'][0]['message']['content'].strip()\n",
        "  # return summary\n",
        "\n",
        "\n",
        "# def keywords(prompt,model = 'gpt-3.5-turbo'):\n",
        "#   # Connect to API endpoint\n",
        "#   URL = 'https://api.openai.com/v1/chat/completions'\n",
        "\n",
        "#   parameters = {\n",
        "#       'model':model,\n",
        "#       'messages': [{'role':'user', 'content':prompt}],\n",
        "#       'temperature': 1,\n",
        "#       'top_p': 1,\n",
        "#       'frequency_penalty':1,\n",
        "#       'presence_penalty':1\n",
        "#   }\n",
        "\n",
        "#   headers = {\n",
        "#       'Content-Type': 'application/json',\n",
        "#       'Authorization': f'Bearer {openai.api_key}'\n",
        "#   }\n",
        "\n",
        "#   response = requests.post(URL, headers=headers, json=parameters, stream=False)\n",
        "#   json_response = response.json()\n",
        "#   summary = json_response['choices'][0]['message']['content'].strip()\n",
        "#   return summary\n",
        "\n",
        "if input_file:\n",
        "#   prompt = f'''\n",
        "#   ### TASK\n",
        "#   You are an expert in text analysis and creating abstracts from research papers. Your task is to analyse a research paper I'm going\n",
        "#   to give you and create an abstract for it. Return a list of 5 research papers that are similar to this paper. Sort the list in descending order\n",
        "#   from most relevant to least relevant.\n",
        "#   ### INPUTS\n",
        "#   Research Paper: {input_file}\n",
        "#   ### OUTPUT\n",
        "#   The abstract must answer the following questions: What is the paper about? Why is the paper important? What methods were used?\n",
        "#   What industry partners were involved? What are the paper's findings? Why are the papers findings important?\n",
        "#   '''\n",
        "#   text_area = summarise_file(prompt)\n",
        "\n",
        "#   # Show Summary\n",
        "#   st.success('ChatGPT summary ready!')\n",
        "#   st.text_area(label =\"\",value= text_area, placeholder=\"Please upload a PDF to get it's summary\", height = 400)\n",
        "#   st.markdown('')\n",
        "\n",
        "\n",
        "#   st.success('Recommendations loading, please wait...')\n",
        "\n",
        "#   prompt = f'''\n",
        "#   ### TASK\n",
        "#   You are an expert in text analysis and keyword extraction. Your task is to analyse a research paper I'm going\n",
        "#   to give you and extract from it the top scientific keywords that are most representative of its content. Then you're going to convert the\n",
        "#   keywords to lower case and search the provided URL and return the top 5 results. The url you will use is\n",
        "#   http://export.arxiv.org/api/query?search_query=all:keyword&start=0&max_results=5. If the keyword is more than one word seperate with a\n",
        "#   + symbol such as machine+learning.\n",
        "#   ### INPUTS\n",
        "#   Summary: {input_file}\n",
        "#   ### OUTPUT\n",
        "#   The list of results you're going to generate should be in descending order from the most relevant to the least relevant.\n",
        "#   '''\n",
        "#   test = keywords(prompt)\n",
        "#   # Show Recommendations\n",
        "#   st.success('Recommendations ready!')\n",
        "#   st.text_area(label =\"\",value= test, placeholder=\"Please upload a PDF to get it's summary\", height = 400)\n",
        "#   st.markdown('')\n",
        "\n",
        "  def read_pdf(pdf_file):\n",
        "    reader = PdfReader(pdf_file)\n",
        "    content = ''.join(page.extract_text() for page in reader.pages)\n",
        "    return content\n",
        "\n",
        "  def get_completion(client, prompt):\n",
        "      return client.messages.create(\n",
        "          model=MODEL_NAME,\n",
        "          max_tokens=2048,\n",
        "          messages=[{\n",
        "              \"role\": 'user', \"content\":  prompt\n",
        "          }]\n",
        "      ).content[0].text\n",
        "\n",
        "  text = read_pdf(input_file)\n",
        "  completion = get_completion(client,\n",
        "    f\"\"\"Here is an academic paper: <paper>{text}</paper>\n",
        "\n",
        "  Please do the following:\n",
        "  1.Summarise the research paper in such a way that the reader won't need to read the whole paper.\n",
        "  2.Return a list of 5 research papers that are similar to this paper. Sort the list in descending order\n",
        "  from most relevant to least relevant. Provide a direct quote from the paper.\n",
        "  \"\"\"\n",
        "  )\n",
        "  st.success('Claude summary ready!')\n",
        "  st.text_area(label =\"\",value= completion, placeholder=\"Please upload a PDF to get it's summary\", height = 400)\n",
        "  st.markdown('')\n",
        "\n",
        "else:\n",
        "  st.text_area(label =\"\",value= \"Please upload a PDF to get it's summary\", placeholder=\"Please upload a PDF to get it's summary\", height = 400)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Set authentication token if you haven't already done so\n",
        "ngrok.set_auth_token(\"\")\n",
        "\n",
        "# Start Streamlit server on a specific port\n",
        "!nohup streamlit run app.py --server.port 5011 &\n",
        "\n",
        "# Start ngrok tunnel to expose the Streamlit server\n",
        "ngrok_tunnel = ngrok.connect(addr='5011', proto='http', bind_tls=True)\n",
        "\n",
        "# Print the URL of the ngrok tunnel\n",
        "print(' * Tunnel URL:', ngrok_tunnel.public_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G53ebAhx6tpq",
        "outputId": "5f925ba5-b8e7-4b36-8113-cd03872f5776"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nohup: appending output to 'nohup.out'\n",
            " * Tunnel URL: https://5134-35-203-184-183.ngrok-free.app\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(ngrok.get_tunnels())\n",
        "ngrok.disconnect(ngrok_tunnel.public_url)\n",
        "print(ngrok.get_tunnels())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-D2kuH1E9RxT",
        "outputId": "424f8834-c656-4fa0-99a8-a47b23a28f28"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:pyngrok.process.ngrok:t=2024-07-25T17:37:21+0000 lvl=warn msg=\"Stopping forwarder\" name=http-5011-c9674a87-6686-444c-9a78-d9a93c918bfe acceptErr=\"failed to accept connection: Listener closed\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[<NgrokTunnel: \"https://5134-35-203-184-183.ngrok-free.app\" -> \"http://localhost:5011\">]\n",
            "[]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LocalTunnel keeps going down: use this instead\n",
        "## https://pyngrok.readthedocs.io/en/latest/integrations.html#colab-ssh-example"
      ],
      "metadata": {
        "id": "fnuP3F8eQdsa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Rerun this if error"
      ],
      "metadata": {
        "id": "T-VT_obdVm7U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!npm install -g localtunnel"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "piAkMTQGN7ZM",
        "outputId": "61a61cd9-0a22-49d7-b22a-133671231b43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K\u001b[?25h\n",
            "added 22 packages, and audited 23 packages in 4s\n",
            "\n",
            "3 packages are looking for funding\n",
            "  run `npm fund` for details\n",
            "\n",
            "1 \u001b[33m\u001b[1mmoderate\u001b[22m\u001b[39m severity vulnerability\n",
            "\n",
            "To address all issues (including breaking changes), run:\n",
            "  npm audit fix --force\n",
            "\n",
            "Run `npm audit` for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl ipv4.icanhazip.com"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8wQvJHmR8-_J",
        "outputId": "67cf3f64-7d46-4c9d-fc6b-a160f2047ea4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.168.84.220\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py &>./logs.txt & npx localtunnel --port 8501"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bf5l7uo19BtD",
        "outputId": "972e7575-76f4-4ce7-d9c5-b3897508d236"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install anthropic -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKSmeuk5NfSt",
        "outputId": "b01edba1-d110-42ec-9ca8-1f1da034b0c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from anthropic import Anthropic\n",
        "client = Anthropic(\n",
        "    api_key=''\n",
        ")\n",
        "MODEL_NAME = \"claude-3-opus-20240229\""
      ],
      "metadata": {
        "id": "1low860SQE5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_completion(client, prompt):\n",
        "    return client.messages.create(\n",
        "        model=MODEL_NAME,\n",
        "        max_tokens=2048,\n",
        "        messages=[{\n",
        "            \"role\": 'user', \"content\":  prompt\n",
        "        }]\n",
        "    ).content[0].text"
      ],
      "metadata": {
        "id": "PEDc5OKOQMwz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = '/content/Operational_range_bounding_of_spectroscopy_models_with_anomaly_detection_draft.pdf'"
      ],
      "metadata": {
        "id": "4w1KwYQrRCTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf -q"
      ],
      "metadata": {
        "id": "qNtJqFPDqvcJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pypdf import PdfReader\n",
        "\n",
        "reader = PdfReader('/content/Operational_range_bounding_of_spectroscopy_models_with_anomaly_detection_draft.pdf')\n",
        "number_of_pages = len(reader.pages)\n",
        "text = ''.join(page.extract_text() for page in reader.pages)\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RQJ_TaJIqyOG",
        "outputId": "0b107a7c-56e6-4799-94f7-6bb9e645aaf2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DRAFT version. Last modified: 2024-05-30 22:03:17Z\n",
            "Operational range bounding of spectroscopy models\n",
            "with anomaly detection\n",
            "Luís F. Simões∗1, Pierluigi Casale1, Marília Felismino1,\n",
            "Kai Hou Yip2, Ingo P . Waldmann2, Giovanna Tinetti2, Theresa Lueftinger3\n",
            "1ML Analytics, Portugal\n",
            "2Department of Physics and Astronomy, University College London, Gower Street, London, WC1E 6BT, UK\n",
            "3European Space Agency, ESA-ESTEC, The Netherlands\n",
            "Safe operation of machine learning models demands ar-\n",
            "chitectures that explicitly delimit models’ operational\n",
            "ranges. We evaluate the capacity of anomaly detection al-\n",
            "gorithms to provide indicators correlated with degraded\n",
            "model performance. Thresholding on such indicators\n",
            "places hard boundaries on a model’s coverage. The use\n",
            "case considered deals with the extraction of exoplane-\n",
            "tary spectra from transiting light curves, specifically in\n",
            "the context of ESA’s upcoming Ariel mission. Isolation\n",
            "Forests modelling the data distribution are found to ac-\n",
            "curately flag contexts under which predictive models\n",
            "trained over that same data will fail. Of the explored ap-\n",
            "proaches, we see the best coverage/error trade-o ffs arising\n",
            "when Isolation Forests model PCA projections of the pre-\n",
            "dictive model’s explainability SHAP values.\n",
            "1 Introduction\n",
            "As Machine Learning adoption grows across risk-\n",
            "averse industries such as aerospace, various e fforts are\n",
            "underway to design the validation processes that will\n",
            "ensure safety in complex operational settings [1–5].\n",
            "The safety cage architecture being advocated by such\n",
            "efforts consists of a safety mechanism that continu-\n",
            "ously monitors system observables, and intervenes\n",
            "if it judges the model will exceed the bounds of a\n",
            "safe domain for a particular input. Unlike monitor-\n",
            "ing systems that run asynchronously from the pre-\n",
            "diction model and perform evaluations over assem-\n",
            "bled production data [6], the safety cage architecture\n",
            "runs a monitoring system in parallel with the predic-\n",
            "tion model, and operates at an individual sample level.\n",
            "This places explicit boundaries over a model’s opera-\n",
            "tional range, something they traditionally lack.\n",
            "Exoplanet transit spectroscopy.\n",
            "Atmospheres of exoplanets are studied using transit\n",
            "spectroscopy. As a planet passes through the projected\n",
            "∗Corresponding author. E-Mail: luis.simoes@mlanalytics.ptsurface of the host star, the observed light curve will\n",
            "display a temporary reduction (a dip) in the overall\n",
            "flux from the planetary system. The level of reduction\n",
            "varies in di fferent wavelength channels, which is key\n",
            "to revealing the chemical and dynamical properties of\n",
            "the planet’s atmosphere. These changes are typically\n",
            "100 ppm or less in magnitude, making them easily\n",
            "buried under di fferent instrumental and astrophysical\n",
            "noises orders of magnitude higher than the signal.\n",
            "The Ariel Mission is a European Space Agency\n",
            "(ESA) M4 mission that will conduct the first compre-\n",
            "hensive study of 1000 exoplanets in our galactic neigh-\n",
            "bourhood [7, 8]. The Ariel Data Challenge [9–11] is a\n",
            "series of data competitions organised by mission sci-\n",
            "entists and engineers within the Ariel Consortium to\n",
            "invite innovative solutions to challenging problems\n",
            "faced by the ESA Ariel mission, and by extension, the\n",
            "Planetary Science community. The 2019 and 2021 edi-\n",
            "tions of the challenge [9] looked for innovative de-\n",
            "trending solutions to extract planetary spectra from\n",
            "observed spectroscopic light curves that are contam-\n",
            "inated by the instrument’s systematic noise e ffects,\n",
            "as well as stellar activities from the host stars. The\n",
            "dataset simulations represent Ariel observations with\n",
            "mission-dedicated radiometric models to ensure good\n",
            "representation of the instrument. The simulations can\n",
            "however never represent the actual data, which will\n",
            "suffer from unknown unknowns. Ensuring the reli-\n",
            "able operation of Machine Learning models, as they\n",
            "move from simulated to real data, with unknowable\n",
            "ground truths, is of paramount importance.\n",
            "2 Results\n",
            "Datasets. We conduct our experiments over the\n",
            "datasets from the 2019 and 2021 Ariel Data Chal-\n",
            "lenges, hereby designated as ADC19 and ADC21 [9].DRAFT version. Last modified: 2024-05-30 22:03:17Z\n",
            "50 100 150 200 250\n",
            "Time0.9900.9920.9940.9960.9981.0001.0021.004Relative fluxADC19 planet 1247Light curve (X′)\n",
            "50 100 150 200 250\n",
            "Time0.9900.9920.9940.9960.9981.0001.0021.004ADC21 planet 1295Light curve (X′)\n",
            "0.5 1 2 5 10\n",
            "Wavelength (m)\n",
            "0.0740.0760.0780.0800.0820.0840.086Rp/Rs Relative radii (planet-to-star-radius ratios)Transmission spectrum (Y)\n",
            "ADC19 planet 1247\n",
            "ADC21 planet 1295\n",
            "Figure 1: Transit light curves for the same star/planet system, produced by the simulation pipelines [12–15] of the ADC19\n",
            "(left) and ADC21 (middle) challenges (seen: 55 light curves, one per wavelength channel, each transformed through a\n",
            "sliding-window aggregation over time and across the 10 photon noise instances of the star’s first stellar spot configuration).\n",
            "Right plot: the planet’s transmission spectrum (55 channels) the trained model needs to predict from the 55 light curves.\n",
            "Simulations for exoplanet Qatar-4b, using star/planet parameters from the Ariel Mission Candidate Sample [16, 17].\n",
            "The 2019 challenge focused solely on stellar noise\n",
            "in the simulations but assumed the instrument was\n",
            "perfect. In 2021 the signal was convolved with the\n",
            "non-linear and time-dependent instrument response\n",
            "models, introducing detector persistence and non-\n",
            "Gaussian red-noise e ffects. From a modelling per-\n",
            "spective, those di fferences enable the assessment of\n",
            "model performance under conditions of data drift\n",
            "(also known as covariate shift) and concept drift [19].\n",
            "We use the train data that was made publicly avail-\n",
            "able for those competitions. They contain simulations\n",
            "of 1468 and 1256 star/planet systems, respectively.\n",
            "Experiments across datasets are restricted to a core\n",
            "of 851 systems that were simulated in both datasets,\n",
            "with the same star/planet parameters (Figure 1). The\n",
            "data generation process simulated 10 di fferent stellar\n",
            "spot configurations per system, which pollute the sig-\n",
            "nal and therefore have to be identified and corrected\n",
            "for in the derivation of the spectra. Each stellar spot\n",
            "configuration was further corrupted by 10 di fferent\n",
            "instances of additive Gaussian photon noise.\n",
            "Data pre-processing pipeline. We treat the photon\n",
            "noise instances as repeated observations of the same\n",
            "system, and aggregate data across them. We make use\n",
            "of the fact that the known planet parameters allow\n",
            "for alignment of independent observations’ time se-\n",
            "ries, leaving us in this case with an expectation that\n",
            "the maximum dip in the transit light curve will oc-\n",
            "cur around the t= 150 time-step. We implement a\n",
            "\"mirror-stacking\" approach where the observation ma-\n",
            "trix is stacked with a time-inverted copy of itself, thus\n",
            "exploiting the light curve’s expected symmetry (at\n",
            "time t= 140 , for instance, we’ll have as well the obser-\n",
            "vations taken at t= 160 ). A regular grid of 11 points\n",
            "is taken across the time series, every 5 time steps be-tween t= 100 andt= 150 . Per grid point, an aggre-\n",
            "gation is performed using a Harmonic mean across\n",
            "the observations in a time window of radius r= 10 ,\n",
            "and across the same windows of the 10 photon noise\n",
            "instances. Each encoded value then results from aggre-\n",
            "gating across (r∗2+1) ∗2∗10 = 420 observations. At this\n",
            "point, data is transformed from relative flux values\n",
            "into relative radii, using the Rp/Rs=p\n",
            "1−min(1,f)\n",
            "relation. This has the advantage of simplifying the\n",
            "modelling task, as both model inputs and outputs will\n",
            "now be in the same space. This process is repeated\n",
            "across observations’ 55 wavelength channels, leaving\n",
            "us with a 11∗55 = 605 dimensional data encoding. Fi-\n",
            "nally, we stack onto this representation the 6 known\n",
            "star and planet parameters (stellar temperature, sur-\n",
            "face gravity, etc.). By following this process, the raw\n",
            "data is transformed into a matrix Xof 12560 611-\n",
            "dimensional samples, in the case of ADC21 data, or\n",
            "8510 samples in experiments across datasets.\n",
            "Modelling pipeline. For the sake of simplicity, and to\n",
            "obtain models having abundant failure contexts, we\n",
            "modelled the problem of extracting transmission spec-\n",
            "tra from the previously described data using Ridge re-\n",
            "gression (linear least squares with L2regularisation).\n",
            "The modelling of data distributions for the purpose\n",
            "of anomaly detection used Isolation Forest models\n",
            "[20, 21]. These models allow us to identify samples\n",
            "that belong to low density region of the training data.\n",
            "Scarcity of data in such regions leads one to assume\n",
            "the predictive model (Rigde in this case) won’t have\n",
            "been su fficiently informed as to how to model the tar-\n",
            "get, and its performance will then be expected to be\n",
            "worse there. Isolation Forests produce anomaly scores\n",
            "for the data samples they are given. Values above 0.0\n",
            "are judged to be inliers with respect to the trainingDRAFT version. Last modified: 2024-05-30 22:03:17Z\n",
            "-0.2 -0.1 0.0 0.1\n",
            "Observation's anomaly score103\n",
            "102\n",
            "101\n",
            "Error (RMSE) across model predictions for\n",
            "the observation's 55 wavelength channelsoutliers    inliersSpearman correlation: -0.56X(611)  PCA(30)  IsolationForest\n",
            "-0.2 -0.1 0.0 0.1\n",
            "Observation's anomaly score103\n",
            "102\n",
            "101\n",
            "outliers    inliersSpearman correlation: -0.52Y(55)  PCA(30)  IsolationForest\n",
            "-0.2 -0.1 0.0 0.1\n",
            "Observation's anomaly score103\n",
            "102\n",
            "101\n",
            "outliers    inliersSpearman correlation: -0.58Xshap(611)  PCA(30)  IsolationForest\n",
            "Figure 2: Relation between anomaly scores produced by Isolation Forests and predictive performance of another model\n",
            "trained over the same data. Results obtained through 10-fold cross-validation over the ADC21 dataset. Plots show values\n",
            "measured across the validation-folds for the whole dataset, from Isolation Forest and Ridge models trained over the re-\n",
            "spective train-folds. In the left plot, the Isolation Forest models the distribution of X, the 611-dimensional representations\n",
            "of light curves the Ridge model takes as inputs. In the central plot the modelled distribution is that of Ytrue, the spectra\n",
            "Ridge models are trained to predict. In the right plot the Isolation Forest models the Xshap values produced by the SHAP\n",
            "Explainable AI library [18] that represent how the Ridge model transforms Xinto Ypred.\n",
            "data, and values below that outliers. As we are deal-\n",
            "ing with high dimensional datasets, Isolation Forests\n",
            "were set to model data projections into a lower 30-\n",
            "dimensional space. Those projections were obtained\n",
            "using Principal Component Analysis (PCA).\n",
            "We assess the e ffectiveness of three di fferent\n",
            "anomaly detection setups, that di ffer in the data Isola-\n",
            "tion Forests, and PCA, were asked to model (Figure 2):\n",
            "•X: default setup, models the data the predictive\n",
            "model takes as input (light curve encodings here).\n",
            "•Ytrue: this setup ignores the input data, and di-\n",
            "rectly models the transmission spectra. The intu-\n",
            "ition being that if the other model is generating\n",
            "outputs unlike anything that exists in the train-\n",
            "ing data, it will then likely be making mistakes.\n",
            "•Xshap: a challenge in tying anomaly scores to an-\n",
            "other model’s predictive performance lies in that\n",
            "the anomaly detection model will lack sensitiv-\n",
            "ity to the kinds of data shifts that most impact\n",
            "the other model. One model treats all dimen-\n",
            "sions as equally important. The other doesn’t. In\n",
            "this setup the Isolation Forest then models the\n",
            "SHAP values (\"SHapley Additive exPlanations\")\n",
            "produced by the SHAP Explainable AI library\n",
            "[18] that represent how the Ridge model trans-\n",
            "forms Xinto Ypred. This being a multi-output re-\n",
            "gression problem, SHAP generates here 55 vec-\n",
            "tors of 611 SHAP values per input sample. The\n",
            "median is taken across the 55 vectors to obtain\n",
            "the data the Isolation Forest is made to model.\n",
            "We used the Ridge, PCA and Isolation Forest im-\n",
            "plementations available in the scikit-learn 0.24.2library [22], with their default parameter settings.\n",
            "SHAP values obtained with the shap 0.39.0 library\n",
            "[18], using a LinearExplainer with interventional\n",
            "feature perturbation.\n",
            "Cross-validation setup. Experiments carried out us-\n",
            "ing Group 10-fold cross-validation. The data pre-\n",
            "processing described above results in datasets with\n",
            "groups of 10 samples belonging to the same planet,\n",
            "having di fferent stellar spot noise configurations.\n",
            "Cross-validation then ensured samples belonging to\n",
            "the same planet would always be together in either\n",
            "a train- or validation-fold. Experiments across the\n",
            "ADC19 and ADC21 datasets, having fewer planets,\n",
            "used instead 5-fold CV to increase the volume and di-\n",
            "versity of validation folds. In those experiments, plan-\n",
            "ets in the train-fold were modelled using encodings of\n",
            "ADC21 data, but planets placed the validation-folds\n",
            "used instead encodings of ADC19 data. This allows us\n",
            "to assess e ffectiveness of the propossed approaches in\n",
            "contexts of degraded model performance due to drift.\n",
            "Data standardisation into zero mean and unit vari-\n",
            "ance was implemented, as these are important pre-\n",
            "processing steps for Ridge and PCA. Standardisation\n",
            "was defined over each train-fold, and then applied\n",
            "there and in the corresponding validation fold.\n",
            "Predictive performance was assessed using the Root\n",
            "Mean Squared Error (RMSE) metric, chosen for its sen-\n",
            "sitivity to large errors, thus expressing the preference\n",
            "for all values in a spectrum to be accurately predicted.\n",
            "Results. With the previously presented setup, Ridge\n",
            "models achieve a RMSE of 0.006085 when cross-\n",
            "validating across the ADC21 dataset, and a degrada-DRAFT version. Last modified: 2024-05-30 22:03:17Z\n",
            "0 20 40 60 80 100\n",
            "Coverage: percentage of test data within model's operational range0.0010.0020.0030.0040.0050.006Error (RMSE) across predictions for the covered samplesCross-validation: train & validate over ADC21 data\n",
            "Anomaly detection setup\n",
            "X(611)  PCA(30)  IsolationForest\n",
            "Y(55)  PCA(30)  IsolationForest\n",
            "Xshap(611)  PCA(30)  IsolationForest\n",
            "0 20 40 60 80 100\n",
            "Coverage: percentage of test data within model's operational range0.0010.0020.0030.0040.0050.0060.0070.008Error (RMSE) across predictions for the covered samplesCross-validation: train over ADC21 data, validate over ADC19\n",
            "Anomaly detection setup\n",
            "X(611)  PCA(30)  IsolationForest\n",
            "Y(55)  PCA(30)  IsolationForest\n",
            "Xshap(611)  PCA(30)  IsolationForest\n",
            "Figure 3: Trade-o ffs achievable between model coverage and predictive performance when acceptance thresholds are\n",
            "defined over the anomaly scores seen in Figure 2.\n",
            "tion to 0.008421 when cross-validation trains over\n",
            "ADC21 and validates over ADC19. The di fferent num-\n",
            "ber of planets across these setups impacts these mea-\n",
            "surements, but they are in line with the performance\n",
            "degradation seen when training over the full ADC21\n",
            "dataset, and then evaluating over the full ADC19\n",
            "dataset, where we observe a RMSE of 0.008589.\n",
            "Figure 2 shows the anomaly scores measured across\n",
            "the validation folds having a moderate Spearman’s\n",
            "rank correlation with the RMSE values taken over the\n",
            "sample’s 55 wavelength channels. The strongest corre-\n",
            "lation is observed in the Xshapsetup, which validates\n",
            "the hypothesis that aligning the modelled distribution\n",
            "with the behaviour of the predictive model improves\n",
            "the capacity to identify performance degradation.\n",
            "Figure 3 shows the e ffectiveness of a Safety Cage\n",
            "architecture that delimits the predictive model’s oper-\n",
            "ational range by imposing acceptance thresholds over\n",
            "the anomaly scores obtained through the approaches\n",
            "presented here. It shows the coverage vs.error trade-\n",
            "offs achievable as that threshold varies across the span\n",
            "of anomaly scores observed in the validation folds.\n",
            "Again we see the Xshapsetup as being the most e ffec-\n",
            "tive one. It presents the slowest curve growth, which\n",
            "corresponds to an operational range boundary that is\n",
            "more accurate at including within it the performant\n",
            "regions, and leaving outside the error-prone ones. The\n",
            "right plot shows that even though the transition to\n",
            "ADC19 data entails a large performance degradation\n",
            "overall, that is due to large errors in samples that are\n",
            "easily flagged by the Isolation Forest. An acceptance\n",
            "threshold that would reduce coverage by a small per-\n",
            "centage would already reduce RMSE by ∼20%.3 Discussion\n",
            "In missions like Ariel, prediction failures by Machine\n",
            "Learning models pose risks, but also opportunities.\n",
            "Data products obtained through ML-assisted process-\n",
            "ing of the mission’s data cannot be contaminated by\n",
            "wrong extrapolations beyond models’ training data.\n",
            "The architecture explored here succeeds at provid-\n",
            "ing safeguards against such risks. We have tackled\n",
            "the problem of identifying out-of-domain samples,\n",
            "which enables us to recognise contexts of high epis-\n",
            "temic uncertainty. As future work we plan to extend\n",
            "the approach with uncertainty quantification meth-\n",
            "ods, which will provide identification as well for con-\n",
            "texts of high aleatoric uncertainty [23].\n",
            "Beyond such risks, there are also opportunities.\n",
            "Anomaly/novelty detection may point to observation\n",
            "targets of particular scientific interest, due to their\n",
            "uniqueness. Current modelling failures are not guar-\n",
            "anteed to remain as failures if subsequent e ffort is al-\n",
            "located to better simulate and understand the failure\n",
            "cases. Some failures result from sparsity in the train-\n",
            "ing data that doesn’t allow the modelling process to\n",
            "capture the patterns of how to process such data. This\n",
            "sort of feedback is valuable to the e ffort of gradually\n",
            "building the best possible simulations of the targets\n",
            "of interest, which improves models’ training datasets.\n",
            "The gaps in coverage left by delimiting a model’s op-\n",
            "erational range may also be filled by di fferent models,\n",
            "potentially of di fferent kinds, that specialise at accu-\n",
            "rate prediction where previous models fail. The cur-\n",
            "rent architecture may also assist such e fforts.DRAFT version. Last modified: 2024-05-30 22:03:17Z\n",
            "Acknowledgement\n",
            "Research funded by the ESA Science Faculty Research\n",
            "projects \"Machine Learning Quality Assurance in\n",
            "Ariel and beyond\" and \"Ariel Machine Learning Data\".\n",
            "References\n",
            "1. Tambon, F. et al. How to certify machine learning based safety-\n",
            "critical systems? A systematic literature review. Automated\n",
            "Software Engineering 29,38 (2022).\n",
            "2. Mamalet, F. et al. White Paper Machine Learning in Certified\n",
            "Systems Research Report (IRT Saint Exupéry ; ANITI, Mar.\n",
            "2021). https://hal.science/hal-03176080 .\n",
            "3. ECSS-European Cooperation for Space Standardization.\n",
            "Space engineering – Machine learning qualification handbook\n",
            "Public Review ECSS-E-HB-40-02A DIR1 (ESA Requirements\n",
            "and Standards Section, May 2023).\n",
            "4. MLEAP Consortium. EASA Research – Machine Learning Ap-\n",
            "plication Approval (MLEAP) interim technical report Horizon\n",
            "Europe research and innovation programme report (Euro-\n",
            "pean Union Aviation Safety Agency, May 2023).\n",
            "5. Borg, M. et al. Safely Entering the Deep: A Review of Verifi-\n",
            "cation and Validation for Machine Learning and a Challenge\n",
            "Elicitation in the Automotive Industry. Journal of Automotive\n",
            "Software Engineering 1,1–19 (1 2019).\n",
            "6. Treveil, M. et al. Introducing MLOps: How to Scale Machine\n",
            "Learning in the Enterprise isbn : 9781492083290 (O’Reilly Me-\n",
            "dia, Incorporated, 2020).\n",
            "7. Tinetti, G. et al. A chemical survey of exoplanets with ARIEL.\n",
            "Experimental astronomy 46,135–209 (2018).\n",
            "8. Tinetti, G. et al. Ariel: Enabling planetary science across light-\n",
            "years. arXiv preprint arXiv:2104.04824 (2021).\n",
            "9. Nikolaou, N. et al. Lessons learned from the 1st Ariel Ma-\n",
            "chine Learning Challenge: Correcting transiting exoplanet\n",
            "light curves for stellar spots. RAS Techniques and Instruments\n",
            "2,695–709 (2023).\n",
            "10. Changeat, Q. & Yip, K. H. ESA-Ariel Data Challenge NeurIPS\n",
            "2022: introduction to exo-atmospheric studies and presenta-\n",
            "tion of the Atmospheric Big Challenge (ABC) Database. RAS\n",
            "Techniques and Instruments 2,45–61 (Jan. 2023).\n",
            "11. Yip, K. H. et al. Lessons Learned from Ariel Data Challenge\n",
            "2022 - Inferring Physical Properties of Exoplanets From Next-\n",
            "Generation Telescopes inProceedings of the NeurIPS 2022 Com-\n",
            "petitions Track 220(PMLR, Nov. 2022), 1–17.\n",
            "12. Morello, G. et al. The ExoTETHyS package: tools for exoplan-\n",
            "etary transits around host stars. The Astronomical Journal 159,\n",
            "75 (2020).\n",
            "13. Waldmann, I. P . et al. Tau-REx I: A next generation retrieval\n",
            "code for exoplanetary atmospheres. The Astrophysical Journal\n",
            "802, 107 (2015).\n",
            "14. Mugnai, L. V., Pascale, E., Edwards, B., Papageorgiou, A. &\n",
            "Sarkar, S. ArielRad: the Ariel radiometric model. Experimen-\n",
            "tal Astronomy 50,303–328 (2020).\n",
            "15. Sarkar, S., Pascale, E., Papageorgiou, A., Johnson, L. J. & Wald-\n",
            "mann, I. ExoSim: the exoplanet observation simulator. Exper-\n",
            "imental Astronomy 51,287–317 (2021).\n",
            "16. Edwards, B., Mugnai, L., Tinetti, G., Pascale, E. & Sarkar, S.\n",
            "An updated study of potential targets for Ariel. The Astronom-\n",
            "ical Journal 157, 242 (2019).17. Edwards, B. & Tinetti, G. The Ariel Target List: The Impact\n",
            "of TESS and the Potential for Characterizing Multiple Planets\n",
            "within a System. The Astronomical Journal 164, 15 (2022).\n",
            "18. Lundberg, S. M. & Lee, S. -I.A Unified Approach to Interpreting\n",
            "Model Predictions inAdvances in Neural Information Processing\n",
            "Systems 30 (eds Guyon, I. et al. ) (2017), 4765–4774.\n",
            "19. Bayram, F., Ahmed, B. S. & Kassler, A. From concept drift to\n",
            "model degradation: An overview on performance-aware drift\n",
            "detectors. Knowledge-Based Systems 245, 108632 (2022).\n",
            "20. Liu, F. T., Ting, K. M. & Zhou, Z. -H.Isolation Forest in2008\n",
            "Eighth IEEE International Conference on Data Mining (2008),\n",
            "413–422.\n",
            "21. Liu, F. T., Ting, K. M. & Zhou, Z. -H. Isolation-Based Anomaly\n",
            "Detection. ACM Trans. Knowl. Discov. Data 6(2012).\n",
            "22. Pedregosa, F. et al. Scikit-learn: Machine Learning in Python.\n",
            "Journal of Machine Learning Research 12,2825–2830 (2011).\n",
            "23. Psaros, A. F., Meng, X., Zou, Z., Guo, L. & Karniadakis, G. E.\n",
            "Uncertainty quantification in scientific machine learning:\n",
            "Methods, metrics, and comparisons. Journal of Computational\n",
            "Physics 477, 111902 (2023).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "completion = get_completion(client,\n",
        "    f\"\"\"Here is an academic paper: <paper>{text}</paper>\n",
        "\n",
        "Please do the following:\n",
        "1. Summarise the research paper in such a way that the reader won't need to read the whole paper.\n",
        "\"\"\"\n",
        ")\n",
        "print(completion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7CNsGFI7QWBc",
        "outputId": "4df12742-9eaf-4c17-b360-b17c0c4d57ca"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title: Operational range bounding of spectroscopy models with anomaly detection\n",
            "\n",
            "Summary:\n",
            "This research paper explores the use of anomaly detection algorithms to delimit the operational ranges of machine learning models, ensuring safe operation in complex settings like aerospace. The authors focus on the use case of extracting exoplanetary spectra from transiting light curves in the context of ESA's upcoming Ariel mission.\n",
            "\n",
            "Key points:\n",
            "1. The \"safety cage\" architecture involves monitoring system observables and intervening if the model exceeds the bounds of a safe domain for a particular input.\n",
            "2. The authors conduct experiments using datasets from the 2019 and 2021 Ariel Data Challenges, which simulate exoplanet transit spectroscopy data with varying levels of instrumental and astrophysical noise.\n",
            "3. They employ a data pre-processing pipeline that aggregates data across photon noise instances and wavelength channels, and a modelling pipeline using Ridge regression for extracting transmission spectra and Isolation Forest models for anomaly detection.\n",
            "4. Three anomaly detection setups are assessed, differing in the data modeled by Isolation Forests and PCA: input data (X), output data (Ytrue), and SHAP values representing the model's transformation of input to output (Xshap).\n",
            "5. Results show that anomaly scores correlate with the predictive model's performance degradation, with the Xshap setup being the most effective in identifying error-prone regions.\n",
            "6. By imposing acceptance thresholds on anomaly scores, the authors demonstrate achievable trade-offs between model coverage and predictive performance.\n",
            "\n",
            "The paper concludes that the proposed architecture succeeds in providing safeguards against prediction failures by machine learning models in missions like Ariel. Anomaly detection not only helps identify risks but also opportunities for improving simulations and models. The authors plan to extend the approach with uncertainty quantification methods in future work.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You are an expert in creating abstracts from research papers and finding similar abstracts. Your task is to analyse a research paper I'm going\n",
        "  to give you and create an abstract that is most representative of its content especially the points made in the first paragraph. You will th\n",
        "  The url you will use is http://export.arxiv.org/api/query?search_query=all:keyword&start=0&max_results=5. If the keyword is more\n",
        "  than one word seperate with a + symbol such as machine+learning."
      ],
      "metadata": {
        "id": "iYChkUyCiBpR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TASK\n",
        "  You are an expert in text analysis and summarisation with research papers. Your task is to analyse a research paper I'm going to give you\n",
        "  and provide a detailed summary that is most representative of its content. You must use the Key Concept Clarity method to read the paper and\n",
        "  highlight all of the key talking points of the paper.\n",
        "  ### INPUTS\n",
        "  Research Paper: {input_file}\n",
        "  ### OUTPUT\n",
        "  The output should be structured so as to identify and concisely explain the main ideas in the document. Each section should consist of the\n",
        "  name of the talking point, a detailed definition of the talking point and a short summary of the talking point.\n",
        "  Aim to provide at least four talking points. Here is how each talking point should be structured:\n",
        "  Name of the talking point\n",
        "  Explanation: Define in detail what is the talking point\n",
        "  Overview: Provide a short summary of the talking point including its challenges ,if any, and findings.\n",
        "  '''"
      ],
      "metadata": {
        "id": "HbqIaaV-q_uK"
      }
    }
  ]
}